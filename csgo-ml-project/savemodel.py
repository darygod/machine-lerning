# save_model.py - VersiÃ³n corregida que maneja valores NaN
import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.impute import SimpleImputer
import os
import warnings

# Suprimir warnings de tipos mixtos
warnings.filterwarnings('ignore')

def entrenar_y_guardar_modelos():
    """
    Entrena los modelos de CS:GO y los guarda para uso en API
    VersiÃ³n mejorada que maneja valores NaN correctamente
    """
    print("ğŸš€ Iniciando entrenamiento y guardado de modelos...")
    
    # 1. Cargar datos con configuraciÃ³n robusta
    print("ğŸ“Š Cargando datos...")
    file_path = "data/Anexo ET_demo_round_traces_2022.csv"
    
    try:
        # Cargar con configuraciÃ³n robusta para manejar tipos mixtos
        df = pd.read_csv(
            file_path, 
            delimiter=';',
            low_memory=False,  # Evita el warning de tipos mixtos
            na_values=['', 'NULL', 'null', 'NaN', 'nan', '-', 'N/A']  # Valores que deben ser tratados como NaN
        )
        print(f"âœ… Datos cargados: {df.shape}")
        
    except FileNotFoundError:
        print("âŒ Archivo no encontrado. Intentando con ruta del directorio actual...")
        file_path = "./Anexo ET_demo_round_traces_2022.csv"
        df = pd.read_csv(
            file_path, 
            delimiter=';',
            low_memory=False,
            na_values=['', 'NULL', 'null', 'NaN', 'nan', '-', 'N/A']
        )
        print(f"âœ… Datos cargados desde directorio actual: {df.shape}")
    
    # 2. DiagnÃ³stico inicial de datos
    print(f"\nğŸ” DIAGNÃ“STICO INICIAL DE DATOS")
    print(f"   ğŸ“Š Forma del dataset: {df.shape}")
    print(f"   ğŸ“‹ Columnas: {df.columns.tolist()}")
    print(f"   ğŸ”¢ Tipos de datos Ãºnicos: {df.dtypes.value_counts().to_dict()}")
    print(f"   â“ Total valores nulos: {df.isnull().sum().sum()}")
    
    # Mostrar columnas con mÃ¡s valores nulos
    missing_summary = df.isnull().sum().sort_values(ascending=False)
    cols_with_missing = missing_summary[missing_summary > 0]
    
    if len(cols_with_missing) > 0:
        print(f"\nğŸ“Š Columnas con valores faltantes:")
        for col, missing_count in cols_with_missing.head(10).items():
            percentage = (missing_count / len(df)) * 100
            print(f"   {col}: {missing_count} ({percentage:.1f}%)")
    
    # 3. Limpieza inicial
    print(f"\nğŸ§¹ LIMPIEZA INICIAL DE DATOS")
    
    # Eliminar columna Ã­ndice si existe
    if 'Unnamed: 0' in df.columns:
        df.drop(columns=['Unnamed: 0'], inplace=True)
        print("   âœ… Eliminada columna 'Unnamed: 0'")
    
    # Verificar columnas crÃ­ticas
    critical_columns = ['Team', 'MatchWinner', 'TimeAlive', 'Survived']
    missing_critical = [col for col in critical_columns if col not in df.columns]
    
    if missing_critical:
        print(f"âŒ Faltan columnas crÃ­ticas: {missing_critical}")
        print(f"ğŸ“‹ Columnas disponibles: {df.columns.tolist()}")
        return None
    
    # 4. ConversiÃ³n y limpieza de tipos de datos
    print(f"\nğŸ”§ CONVERSIÃ“N DE TIPOS DE DATOS")
    
    # Convertir columnas numÃ©ricas problemÃ¡ticas
    numeric_columns = ['TimeAlive', 'TravelledDistance', 'FirstKillTime']
    
    for col in numeric_columns:
        if col in df.columns:
            print(f"   ğŸ”„ Procesando {col}...")
            
            # Convertir a string primero para limpiar
            df[col] = df[col].astype(str)
            
            # Limpiar caracteres problemÃ¡ticos
            df[col] = df[col].str.replace(',', '.')  # Comas por puntos
            df[col] = df[col].str.replace(' ', '')   # Eliminar espacios
            df[col] = df[col].replace(['nan', 'NaN', 'NULL', ''], np.nan)  # Estandarizar NaN
            
            # Convertir a numÃ©rico
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Reportar estadÃ­sticas
            non_null_count = df[col].count()
            print(f"      âœ… {col}: {non_null_count}/{len(df)} valores vÃ¡lidos")
    
    # Verificar columnas booleanas/categÃ³ricas
    categorical_columns = ['Team', 'MatchWinner', 'Survived', 'AbnormalMatch']
    
    for col in categorical_columns:
        if col in df.columns:
            print(f"   ğŸ·ï¸  {col}: {df[col].value_counts().to_dict()}")
    
    # 5. SelecciÃ³n de caracterÃ­sticas robusta
    print(f"\nğŸ¯ SELECCIÃ“N DE CARACTERÃSTICAS")
    
    # CaracterÃ­sticas principales que normalmente estÃ¡n disponibles
    base_features = [
        'InternalTeamId', 'MatchId', 'RoundId', 'TravelledDistance',
        'RLethalGrenadesThrown', 'RNonLethalGrenadesThrown',
        'RoundKills', 'RoundAssists', 'RoundHeadshots',
        'FirstKillTime'
    ]
    
    # CaracterÃ­sticas adicionales opcionales
    optional_features = [
        'PrimaryAssaultRifle', 'PrimarySniperRifle', 'PrimaryHeavy',
        'PrimarySMG', 'PrimaryPistol', 'MatchKills', 'MatchAssists',
        'MatchHeadshots', 'RoundStartingEquipmentValue', 'TeamStartingEquipmentValue'
    ]
    
    # Encontrar caracterÃ­sticas disponibles
    available_features = []
    
    for feature_list, list_name in [(base_features, "base"), (optional_features, "optional")]:
        for feature in feature_list:
            if feature in df.columns:
                # Verificar que la columna tenga datos Ãºtiles
                non_null_ratio = df[feature].count() / len(df)
                if non_null_ratio > 0.5:  # Al menos 50% de datos no nulos
                    available_features.append(feature)
                    print(f"   âœ… {feature} ({list_name}) - {non_null_ratio:.1%} datos vÃ¡lidos")
                else:
                    print(f"   âš ï¸ {feature} ({list_name}) - Solo {non_null_ratio:.1%} datos vÃ¡lidos, excluido")
    
    print(f"\nğŸ“‹ CaracterÃ­sticas seleccionadas: {len(available_features)}")
    
    # 6. Crear variables objetivo
    print(f"\nğŸ¯ CREANDO VARIABLES OBJETIVO")
    
    df_model = df.copy()
    
    # Variable de clasificaciÃ³n: Supervivencia
    if 'Survived' in df_model.columns:
        # Limpiar la columna Survived
        df_model['Survived'] = df_model['Survived'].fillna(False)  # Asumir que NaN = no sobreviviÃ³
        y_classification = df_model['Survived'].astype(int)
        print(f"   âœ… Usando columna 'Survived' para clasificaciÃ³n")
        print(f"   ğŸ“Š DistribuciÃ³n: {y_classification.value_counts().to_dict()}")
    else:
        # Crear basada en TimeAlive
        threshold = df_model['TimeAlive'].median()
        y_classification = (df_model['TimeAlive'] > threshold).astype(int)
        print(f"   âœ… Creando supervivencia basada en TimeAlive > {threshold:.2f}")
        print(f"   ğŸ“Š DistribuciÃ³n: {y_classification.value_counts().to_dict()}")
    
    # Variable de regresiÃ³n: TimeAlive para sobrevivientes
    survival_mask = (df_model['TimeAlive'] > 0) & (df_model['TimeAlive'].notna())
    y_regression = df_model.loc[survival_mask, 'TimeAlive']
    
    print(f"   ğŸ“ˆ Datos de regresiÃ³n: {len(y_regression)} muestras")
    
    if len(y_regression) < 100:
        print(f"   âš ï¸ Muy pocas muestras para regresiÃ³n ({len(y_regression)})")
        print(f"   ğŸ”„ Intentando usar variable alternativa...")
        
        # Buscar variables alternativas para regresiÃ³n
        alternate_targets = ['RoundKills', 'MatchKills', 'RoundStartingEquipmentValue']
        
        for alt_target in alternate_targets:
            if alt_target in df_model.columns:
                alt_values = df_model[alt_target].dropna()
                if len(alt_values) > 1000:  # Suficientes datos
                    y_regression = alt_values
                    survival_mask = df_model[alt_target].notna()
                    print(f"   âœ… Usando {alt_target} como variable objetivo de regresiÃ³n")
                    break
    
    if len(y_regression) > 0:
        print(f"   ğŸ“Š Variable regresiÃ³n - Min: {y_regression.min():.2f}, Max: {y_regression.max():.2f}, Media: {y_regression.mean():.2f}")
    else:
        print(f"   âŒ No hay datos suficientes para regresiÃ³n")
    
    # 7. CodificaciÃ³n de variables categÃ³ricas
    print(f"\nğŸ·ï¸ CODIFICACIÃ“N DE VARIABLES CATEGÃ“RICAS")
    
    # Codificar Team
    if 'Team' in df_model.columns:
        unique_teams = df_model['Team'].dropna().unique()
        team_mapping = {team: i for i, team in enumerate(unique_teams)}
        df_model['Team_Encoded'] = df_model['Team'].map(team_mapping).fillna(0)
        available_features.append('Team_Encoded')
        print(f"   âœ… Team codificado: {team_mapping}")
    
    # Codificar MatchWinner (maneja NaN)
    if 'MatchWinner' in df_model.columns:
        # Rellenar NaN antes de convertir a entero
        df_model['MatchWinner_Encoded'] = df_model['MatchWinner'].fillna(False).astype(int)
        available_features.append('MatchWinner_Encoded')
        nan_count = df_model['MatchWinner'].isnull().sum()
        print(f"   âœ… MatchWinner codificado ({nan_count} NaN convertidos a 0)")
        
        # Mostrar distribuciÃ³n final
        distribution = df_model['MatchWinner_Encoded'].value_counts().to_dict()
        print(f"   ğŸ“Š DistribuciÃ³n final MatchWinner_Encoded: {distribution}")
    
    # 8. PreparaciÃ³n final de datos
    print(f"\nğŸ”„ PREPARACIÃ“N FINAL DE DATOS")
    
    # Seleccionar caracterÃ­sticas
    X = df_model[available_features].copy()
    
    # Eliminar filas donde la variable objetivo es nula
    valid_classification_mask = y_classification.notna()
    X_classification = X[valid_classification_mask]
    y_classification_clean = y_classification[valid_classification_mask]
    
    # Inicializar variable de control para regresiÃ³n
    has_regression_data = False
    
    # Para regresiÃ³n, usar solo datos con variable objetivo vÃ¡lida
    if len(y_regression) > 100:  # Solo si hay suficientes datos
        X_regression = X[survival_mask]
        y_regression_clean = y_regression
        
        print(f"   ğŸ“Š Datos clasificaciÃ³n finales: {X_classification.shape}")
        print(f"   ğŸ“Š Datos regresiÃ³n finales: {X_regression.shape}")
        
        has_regression_data = True
    else:
        print(f"   âš ï¸ Datos insuficientes para regresiÃ³n ({len(y_regression)} muestras)")
        print(f"   ğŸ”„ Solo se entrenarÃ¡ modelo de clasificaciÃ³n")
        
        # Crear datos vacÃ­os para regresiÃ³n (para mantener compatibilidad)
        X_regression = X_classification[:100]  # Datos dummy
        y_regression_clean = pd.Series(range(100))  # Datos dummy
        has_regression_data = False
    
    # 9. ImputaciÃ³n de valores faltantes
    print(f"\nğŸ”§ IMPUTACIÃ“N DE VALORES FALTANTES")
    
    # Crear imputadores
    imputer_classification = SimpleImputer(strategy='median')
    imputer_regression = SimpleImputer(strategy='median')
    
    # Imputar datos de clasificaciÃ³n
    print(f"   ğŸ“Š NaN en datos de clasificaciÃ³n antes: {X_classification.isnull().sum().sum()}")
    X_classification_imputed = imputer_classification.fit_transform(X_classification)
    print(f"   âœ… ImputaciÃ³n completada para clasificaciÃ³n")
    
    # Imputar datos de regresiÃ³n (solo si hay datos vÃ¡lidos)
    if has_regression_data and len(X_regression) > 0:
        print(f"   ğŸ“Š NaN en datos de regresiÃ³n antes: {X_regression.isnull().sum().sum()}")
        X_regression_imputed = imputer_regression.fit_transform(X_regression)
        print(f"   âœ… ImputaciÃ³n completada para regresiÃ³n")
        
        # Verificar que no queden NaN en regresiÃ³n
        assert not np.isnan(X_regression_imputed).any(), "TodavÃ­a hay NaN en datos de regresiÃ³n"
    else:
        # Crear datos dummy para mantener compatibilidad
        print(f"   âš ï¸ Sin datos de regresiÃ³n vÃ¡lidos, creando datos dummy")
        X_regression_imputed = X_classification_imputed[:100]  # Datos dummy
        imputer_regression = imputer_classification  # Usar el mismo imputador
    
    # Verificar que no queden NaN
    assert not np.isnan(X_classification_imputed).any(), "TodavÃ­a hay NaN en datos de clasificaciÃ³n"
    print(f"   âœ… VerificaciÃ³n: No quedan valores NaN en clasificaciÃ³n")
    
    # 10. DivisiÃ³n de datos
    print(f"\nğŸ“Š DIVISIÃ“N DE DATOS")
    
    # ClasificaciÃ³n
    X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
        X_classification_imputed, y_classification_clean, 
        test_size=0.2, random_state=42, 
        stratify=y_classification_clean if len(np.unique(y_classification_clean)) > 1 else None
    )
    
    # RegresiÃ³n
    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
        X_regression_imputed, y_regression_clean, 
        test_size=0.2, random_state=42
    )
    
    print(f"   âœ… Entrenamiento clasificaciÃ³n: {X_train_clf.shape}")
    print(f"   âœ… Prueba clasificaciÃ³n: {X_test_clf.shape}")
    print(f"   âœ… Entrenamiento regresiÃ³n: {X_train_reg.shape}")
    print(f"   âœ… Prueba regresiÃ³n: {X_test_reg.shape}")
    
    # 11. Escalado de datos
    print(f"\nâš–ï¸ ESCALADO DE DATOS")
    
    scaler_clf = StandardScaler()
    scaler_reg = StandardScaler()
    
    X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)
    X_test_clf_scaled = scaler_clf.transform(X_test_clf)
    
    # Escalado para regresiÃ³n (solo si hay datos vÃ¡lidos)
    if has_regression_data:
        X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)
        X_test_reg_scaled = scaler_reg.transform(X_test_reg)
    else:
        # Usar los mismos datos de clasificaciÃ³n como dummy
        X_train_reg_scaled = X_train_clf_scaled[:50]
        X_test_reg_scaled = X_test_clf_scaled[:20]
        scaler_reg = scaler_clf  # Usar el mismo scaler
    
    print(f"   âœ… Escalado completado")
    
    # VerificaciÃ³n final de NaN
    datasets_to_check = [("X_train_clf_scaled", X_train_clf_scaled)]
    if has_regression_data:
        datasets_to_check.append(("X_train_reg_scaled", X_train_reg_scaled))
    
    for name, data in datasets_to_check:
        if np.isnan(data).any():
            print(f"âŒ ERROR: {name} contiene NaN despuÃ©s del escalado")
            return None
        else:
            print(f"   âœ… {name}: Sin valores NaN")
    
    # 12. Entrenamiento de modelos
    print(f"\nğŸ¤– ENTRENAMIENTO DE MODELOS")
    
    # Modelos de clasificaciÃ³n (solo Random Forest por robustez)
    print(f"   ğŸ”µ Entrenando Random Forest Classifier...")
    rf_classifier = RandomForestClassifier(
        n_estimators=100, 
        random_state=42, 
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2
    )
    rf_classifier.fit(X_train_clf_scaled, y_train_clf)
    rf_clf_score = rf_classifier.score(X_test_clf_scaled, y_test_clf)
    print(f"      âœ… Random Forest Classifier - Accuracy: {rf_clf_score:.4f}")
    
    # Intentar Logistic Regression con configuraciÃ³n robusta
    print(f"   ğŸ”µ Entrenando Logistic Regression...")
    try:
        logistic_classifier = LogisticRegression(
            random_state=42, 
            max_iter=2000,
            solver='liblinear',  # MÃ¡s robusto para datasets problemÃ¡ticos
            C=1.0
        )
        logistic_classifier.fit(X_train_clf_scaled, y_train_clf)
        log_clf_score = logistic_classifier.score(X_test_clf_scaled, y_test_clf)
        print(f"      âœ… Logistic Regression - Accuracy: {log_clf_score:.4f}")
    except Exception as e:
        print(f"      âš ï¸ Logistic Regression fallÃ³: {e}")
        print(f"      ğŸ”„ Usando solo Random Forest para clasificaciÃ³n")
        logistic_classifier = rf_classifier  # Fallback
        log_clf_score = rf_clf_score
    
    # Modelos de regresiÃ³n (solo si hay datos suficientes)
    if has_regression_data:
        print(f"   ğŸ”´ Entrenando Random Forest Regressor...")
        rf_regressor = RandomForestRegressor(
            n_estimators=100, 
            random_state=42, 
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2
        )
        rf_regressor.fit(X_train_reg_scaled, y_train_reg)
        rf_reg_score = rf_regressor.score(X_test_reg_scaled, y_test_reg)
        print(f"      âœ… Random Forest Regressor - RÂ²: {rf_reg_score:.4f}")
        
        print(f"   ğŸ”´ Entrenando Linear Regression...")
        linear_regressor = LinearRegression()
        linear_regressor.fit(X_train_reg_scaled, y_train_reg)
        linear_reg_score = linear_regressor.score(X_test_reg_scaled, y_test_reg)
        print(f"      âœ… Linear Regression - RÂ²: {linear_reg_score:.4f}")
    else:
        print(f"   âš ï¸ Saltando entrenamiento de regresiÃ³n (datos insuficientes)")
        # Crear modelos dummy que devuelvan valores predeterminados
        rf_regressor = type('DummyRegressor', (), {
            'predict': lambda self, X: np.full(len(X), 50.0),
            'score': lambda self, X, y: 0.0
        })()
        linear_regressor = rf_regressor
        rf_reg_score = 0.0
        linear_reg_score = 0.0
    
    # 13. Guardar modelos
    print(f"\nğŸ’¾ GUARDANDO MODELOS")
    
    models_dir = "saved_models"
    os.makedirs(models_dir, exist_ok=True)
    
    # Crear mapeo para la API
    team_mapping = {}
    if 'Team' in df_model.columns:
        unique_teams = df_model['Team'].dropna().unique()
        team_mapping = {team: i for i, team in enumerate(unique_teams)}
    else:
        team_mapping = {'Terrorist': 0, 'Counter-Terrorist': 1}
    
    model_package = {
        'models': {
            'rf_classifier': rf_classifier,
            'logistic_classifier': logistic_classifier,
            'rf_regressor': rf_regressor,
            'linear_regressor': linear_regressor
        },
        'scalers': {
            'scaler_classification': scaler_clf,
            'scaler_regression': scaler_reg
        },
        'imputers': {
            'imputer_classification': imputer_classification,
            'imputer_regression': imputer_regression
        },
        'features': {
            'available_features': available_features,
            'categorical_mapping': {
                'Team': team_mapping,
                'MatchWinner': {False: 0, True: 1}
            }
        },
        'metadata': {
            'training_samples_clf': len(X_train_clf),
            'training_samples_reg': len(X_train_reg) if has_regression_data else 0,
            'feature_count': len(available_features),
            'rf_clf_accuracy': rf_clf_score,
            'log_clf_accuracy': log_clf_score,
            'rf_reg_r2': rf_reg_score,
            'linear_reg_r2': linear_reg_score,
            'data_source': file_path,
            'team_mapping': team_mapping,
            'has_valid_regression': has_regression_data
        }
    }
    
    # Guardar paquete completo
    joblib.dump(model_package, f"{models_dir}/csgo_model_package.pkl")
    print(f"   âœ… Paquete principal guardado: csgo_model_package.pkl")
    
    # Guardar informaciÃ³n adicional
    with open(f"{models_dir}/features.txt", 'w') as f:
        for feature in available_features:
            f.write(f"{feature}\n")
    
    import json
    config = {
        "model_version": "1.0.0",
        "training_date": pd.Timestamp.now().isoformat(),
        "features_count": len(available_features),
        "models_performance": {
            "rf_classifier_accuracy": rf_clf_score,
            "logistic_classifier_accuracy": log_clf_score,
            "rf_regressor_r2": rf_reg_score if has_regression_data else 0.0,
            "linear_regressor_r2": linear_reg_score if has_regression_data else 0.0
        },
        "data_info": {
            "total_samples": len(df),
            "classification_samples": len(X_classification),
            "regression_samples": len(X_regression) if has_regression_data else 0,
            "features": available_features,
            "has_valid_regression": has_regression_data
        }
    }
    
    with open(f"{models_dir}/model_config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"   âœ… ConfiguraciÃ³n guardada: model_config.json")
    print(f"   âœ… Lista de caracterÃ­sticas: features.txt")
    
    # 14. Resumen final
    print(f"\nğŸ‰ Â¡ENTRENAMIENTO COMPLETADO EXITOSAMENTE!")
    print(f"=" * 50)
    print(f"ğŸ“Š RESUMEN FINAL:")
    print(f"   ğŸ¯ CaracterÃ­sticas utilizadas: {len(available_features)}")
    print(f"   ğŸ“ˆ Muestras clasificaciÃ³n: {len(X_train_clf):,}")
    if has_regression_data:
        print(f"   ğŸ“ˆ Muestras regresiÃ³n: {len(X_train_reg):,}")
        print(f"   ğŸ† Mejor regresor: {'Random Forest' if rf_reg_score > linear_reg_score else 'Linear'} ({max(rf_reg_score, linear_reg_score):.3f} RÂ²)")
    else:
        print(f"   âš ï¸ Sin datos suficientes para regresiÃ³n")
    print(f"   ğŸ† Mejor clasificador: Random Forest ({rf_clf_score:.1%})")
    print(f"   ğŸ’¾ Archivos guardados en: {models_dir}/")
    
    print(f"\nğŸš€ PRÃ“XIMO PASO:")
    print(f"   python app.py")
    
    return model_package

if __name__ == "__main__":
    entrenar_y_guardar_modelos()